<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Davide</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Davide</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Sun, 07 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>STDS Assessment 2C</title>
      <link>/blog/2018-10/stds-assessment-2c/</link>
      <pubDate>Sun, 07 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10/stds-assessment-2c/</guid>
      <description>Project Review and Contribution to Community   ‘High Voltage’ Group Dynamics   Paul Robertson Edward Truong Mutaz Abu Ghazzaleh Justin White Davide Lorino (myself)  Working with these four individuals in the process of delivering Assessment 2 (and a variety of side projects - see the ‘Apps’ subheading of ‘Contribution to Statistical Thinking Community’ below) was an immense pleasure. Having previously worked with Edward and Paul in Data Science for Innovation and taken Data Algorithms and Meaning with Mutaz (having had many conversations about statistical concepts together) and also working with somebody I hadn’t met yet - Justin White - I was really excited to take on this project together.
  Group Highlights Working with this group was great because everybody had lots to contribute to the group discussions. Edward was an excellent custodian of our master document and would always keep us on-task with our specific component of the project. Paul had fantastic contributions with his modelling of the data as a timeseries. Mutaz made huge breakthroughs with our analysis when he discovered that our data more closely resembled a polynomial distributions, and Justin acquired the temperature data that would ultimately play a significant role in explaining the variance in energy demand.</description>
    </item>
    
    <item>
      <title>STDS Reflection - Scraping the Australian Energy Market Operator &#39;Energy Demand&#39; Data</title>
      <link>/blog/2018-10/scraping-the-australian-energy-market-operator-energy-demand-data/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10/scraping-the-australian-energy-market-operator-energy-demand-data/</guid>
      <description>In this post we will examine how to download all of the data energy demand data available from the Australian Energy Market Operator by using their aemo package in R. This data is displayed via a graphic on their page, however the graphic only allows for a specific timeframe of 5 days to be viewed at a time, and only one state at a time too. Thankfully the aemo package will let us download all of it! Let’s load the necessary libraries.
library(parsedate) library(aemo) library(tidyverse) library(gridExtra) library(plotly) library(caret) library(htmldeps) The most important function we will use from the aemo package is the collate_aemo_data() function. This will start the process of downloading the data to a directory of your choice.
collate_aemo_data(path = &amp;quot;~/Downloads/New Folder With Items&amp;quot;, remove_files = FALSE) Unfortunately for us, the function does this in the form of different csv files for every single day within the dataset (there are close to 500 csv files that are downloaded). There is also a remove files argument in the collate_aemo_data call, and this is because there used to be a function in the aemo package that would bind the csv files together into one csv file and then delete all of the smaller ones with remove files = TRUE.</description>
    </item>
    
    <item>
      <title>Text Analysis with tidytext, ggplot2 and Quanteda </title>
      <link>/blog/2018-07/text-analysis-with-tidytext-ggplot2-and-quanteda/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/text-analysis-with-tidytext-ggplot2-and-quanteda/</guid>
      <description>In this short guide we’ll perform text analysis on a corpus of 34 blog posts from one blog and try to determine what the blog is about, let’s see how much we can find out without going through and reading all 34 blog posts.
Libraries There are a fair few libraries to load for text analysis;
library(stringr) library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ readr 1.1.1 ## ✔ tibble 1.4.2 ✔ purrr 0.2.5 ## ✔ tidyr 0.8.1 ✔ dplyr 0.7.5 ## ✔ ggplot2 3.0.0 ✔ forcats 0.3.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(tidytext) library(readr) library(tidyr) library(igraph) ## ## Attaching package: &amp;#39;igraph&amp;#39; ## The following objects are masked from &amp;#39;package:dplyr&amp;#39;: ## ## as_data_frame, groups, union ## The following objects are masked from &amp;#39;package:purrr&amp;#39;: ## ## compose, simplify ## The following object is masked from &amp;#39;package:tidyr&amp;#39;: ## ## crossing ## The following object is masked from &amp;#39;package:tibble&amp;#39;: ## ## as_data_frame ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## decompose, spectrum ## The following object is masked from &amp;#39;package:base&amp;#39;: ## ## union library(ggraph)  Load the corpus This file is available on my github.</description>
    </item>
    
    <item>
      <title>Text Analysis with visNetwork</title>
      <link>/blog/2018-07/text-analysis-with-visnetwork/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/text-analysis-with-visnetwork/</guid>
      <description>Text Analysis is growing in popularity just about everywhere - it’s an abundant source of data that until recently most analysts have ignored because of it’s unwieldy structure. Recent developments and packages in the R programming language have made it easy to derive significant meaning from a text corpus.
Practical Applications of Text Analysis  Analysing email data (e.g. from a customer service inbox to analyze queries and feedback), Web scraping (e.g. Cambridge Analytica, though always consider the ethical implications of such methods even if the data is publicly available or rightfully yours to share), Literary analysis (e.g. poems (Shelley vs Byron) and speeches (Obama vs Trump))  In this post we’ll look at the Australian Election Speeches 1901 - 2016 dataset provided by https://electionspeeches.moadoph.gov.au/speeches - we’ll connect R to their API to read the data, then we’ll clean, analyze and visualize the data with some helpful R packages.
library(jsonlite) library(readr) library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union  Connecting to the API Running the line below will pull the speeches data directly from the API of the Museum of Australian Democracy</description>
    </item>
    
    <item>
      <title>Maps with R and Leaflet</title>
      <link>/blog/2018-07/maps-with-r-and-leaflet/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/maps-with-r-and-leaflet/</guid>
      <description>Interactive Maps in R with leaflet 2018-07-24
Plotting interactive maps is easy with the leaflet package in R. In this short guide we’ll use the Launceston Public Seating dataset to plot markers, add pop-ups and freeze the map on an area of interest. Let’s get started.
Load the leaflet and dplyr libraries:
library(leaflet) library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union Now lets look at our dataset.
Launceston Seats On the 20th of July, 2018, the City of Launceston, Tasmania released the ‘Public Seating’ dataset containing 1011 public seats complete with geo-coordinates, materials (aluminium, timber, etc.) and type (seat with back, seat without back, bench, etc.)
The dataset can be found here https://data.gov.au/dataset/public-seating-1c0ab
Import the data into your R environment:
seats_data &amp;lt;- read.csv(url(&amp;quot;http://data-launceston.opendata.arcgis.com/datasets/e4c2e98c2aed4be688ccf9e901deb84a_0.csv&amp;quot;)) Call str() on the dataset and take a look and what we’ve got (i’m ommitting the 13th variable in the dataset because it’s an obscurely layered list of metadata - it won’t be useful for our purposes):
str(seats_data[,1:12]) ## &amp;#39;data.frame&amp;#39;: 990 obs. of 12 variables: ## $ X : num 147 147 147 147 147 .</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/blog/2015-07/hello-r-markdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/blog/2015-07/hello-r-markdown/</guid>
      <description>R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.
You can embed an R code chunk like this:
 </description>
    </item>
    
  </channel>
</rss>