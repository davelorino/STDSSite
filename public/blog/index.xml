<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Davide</title>
    <link>/blog/</link>
    <description>Recent content in Blog on Davide</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Thu, 13 Sep 2018 22:31:21 +0800</lastBuildDate>
    
	<atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ARIMA Timeseries Forecasting in R </title>
      <link>/blog/2018-10/arima-timeseries-forecasting-in-r/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10/arima-timeseries-forecasting-in-r/</guid>
      <description>Auto Regressive Integration of Moving Averages This report aims to explain the desired outcome of the ‘Auto Regressive Integration of Moving Averages’ (ARIMA) model in it’s application towards forecasting energy demand in Australia, as well as the assumptions and limitations that are present through the use of an ARIMA model. ARIMA models are a widely used industry standard for timeseries forecasting.
Definition  A timeseries which needs to be differenced to be made stationary is an “integrated” (I) series. Lags of the stationarized series are called “auto-regressive” (AR) terms. Lags of the forecast errors are called “moving average” (MA) terms.   A Very Short History ARIMA models have risen to prominence in recent times though they are an adaptation of a discrete-time filtering method developed in the 1930s by electrical engineers (Norbert Weiner et al). Since then, statisticians George Box and Gwilym Jenkins developed systematic methods for applying them to business and economic data in the 1970s (ARIMA models are also sometimes referred to as “Box-Jenkins” models).
 ARIMA Terminology A non-seasonal ARIMA model is summarized by three key terms:
 p = the number of autoregressive terms d = the number of nonseasonal differences q = the number of moving-average terms  This is called an “ARIMA(p,d,q)” model</description>
    </item>
    
    <item>
      <title>STDS Assessment 2C</title>
      <link>/blog/2018-10/stds-assessment-2c/</link>
      <pubDate>Sun, 07 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10/stds-assessment-2c/</guid>
      <description>Project Review and Contribution to Community   ‘High Voltage’ Group Dynamics   Paul Robertson Edward Truong Mutaz Abu Ghazzaleh Justin White Davide Lorino (myself)  Working with these four individuals in the process of delivering Assessment 2 (and a variety of side projects - see the ‘Apps’ subheading of ‘Contribution to Statistical Thinking Community’ below) was an immense pleasure. Having previously worked with Edward and Paul in Data Science for Innovation and taken Data Algorithms and Meaning with Mutaz (having had many conversations about statistical concepts together) and also working with somebody I hadn’t met yet - Justin White - I was really excited to take on this project together.
  Group Highlights Working with this group was great because everybody had lots to contribute to the group discussions. Edward was an excellent custodian of our master document and would always keep us on-task with our specific component of the project. Paul had fantastic contributions with his modelling of the data as a timeseries. Mutaz made huge breakthroughs with our analysis when he discovered that our data more closely resembled a polynomial distributions, and Justin acquired the temperature data that would ultimately play a significant role in explaining the variance in energy demand.</description>
    </item>
    
    <item>
      <title>STDS Reflection - Scraping the Australian Energy Market Operator &#39;Energy Demand&#39; Data</title>
      <link>/blog/2018-10/scraping-the-australian-energy-market-operator-energy-demand-data/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10/scraping-the-australian-energy-market-operator-energy-demand-data/</guid>
      <description>In this post we will examine how to download all of the data energy demand data available from the Australian Energy Market Operator by using their aemo package in R. This data is displayed via a graphic on their page, however the graphic only allows for a specific timeframe of 5 days to be viewed at a time, and only one state at a time too. Thankfully the aemo package will let us download all of it! Let’s load the necessary libraries.
library(parsedate) library(aemo) library(tidyverse) library(gridExtra) library(plotly) library(caret) library(htmldeps) The most important function we will use from the aemo package is the collate_aemo_data() function. This will start the process of downloading the data to a directory of your choice.
collate_aemo_data(path = &amp;quot;~/Downloads/New Folder With Items&amp;quot;, remove_files = FALSE) Unfortunately for us, the function does this in the form of different csv files for every single day within the dataset (there are close to 500 csv files that are downloaded). There is also a remove files argument in the collate_aemo_data call, and this is because there used to be a function in the aemo package that would bind the csv files together into one csv file and then delete all of the smaller ones with remove files = TRUE.</description>
    </item>
    
    <item>
      <title>Automate the Browser with Selenium and Python</title>
      <link>/blog/2018-08/automate-the-browser-with-selenium-and-python/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-08/automate-the-browser-with-selenium-and-python/</guid>
      <description>Q: How do you scrape data off a website that’s rendered in Javascript?  A: The Selenium module in Python. In this post I will go over he can use Selenium in Python to interact with website objects that are rendered in javascript so that we can do things like:
 Hover-over dropdown menus Select sub-menus Fill in text entry fields such as username and password to perform login operations Basically use the .click() and .find_element_by_xpath() functions to do everything  Building a webscraper/crawler to programmatically pull data from the internet is a really commonly sought after skill. Most businesspeople put together reports that are usually some sort of aggregation on multiple csv files that all need to get pooled together and joined in a summarized form.
Because of the iterative nature of these reports (daily, weekly, monthly, quarterly) the process of requesting and pooling the data together can be extremely tedious, especially if you have lots of data in different places.
For this, we need the Selenium module in Python. We can pre-generate a script that will fetch our data each and every time we want to run a report - the only thing that we will change in our script between iterations will be the date variables so that we arent just pulling data for the exact same date or range every time (we will want different data otherwise theres no point doing more than one of the same report).</description>
    </item>
    
    <item>
      <title>Text Analysis with tidytext, ggplot2 and Quanteda </title>
      <link>/blog/2018-07/text-analysis-with-tidytext-ggplot2-and-quanteda/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/text-analysis-with-tidytext-ggplot2-and-quanteda/</guid>
      <description>In this short guide we’ll perform text analysis on a corpus of 34 blog posts from one blog and try to determine what the blog is about, let’s see how much we can find out without going through and reading all 34 blog posts.
Libraries There are a fair few libraries to load for text analysis;
library(stringr) library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ readr 1.1.1 ## ✔ tibble 1.4.2 ✔ purrr 0.2.5 ## ✔ tidyr 0.8.1 ✔ dplyr 0.7.5 ## ✔ ggplot2 3.0.0 ✔ forcats 0.3.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(tidytext) library(readr) library(tidyr) library(igraph) ## ## Attaching package: &amp;#39;igraph&amp;#39; ## The following objects are masked from &amp;#39;package:dplyr&amp;#39;: ## ## as_data_frame, groups, union ## The following objects are masked from &amp;#39;package:purrr&amp;#39;: ## ## compose, simplify ## The following object is masked from &amp;#39;package:tidyr&amp;#39;: ## ## crossing ## The following object is masked from &amp;#39;package:tibble&amp;#39;: ## ## as_data_frame ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## decompose, spectrum ## The following object is masked from &amp;#39;package:base&amp;#39;: ## ## union library(ggraph)  Load the corpus This file is available on my github.</description>
    </item>
    
    <item>
      <title>Text Analysis with visNetwork</title>
      <link>/blog/2018-07/text-analysis-with-visnetwork/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/text-analysis-with-visnetwork/</guid>
      <description>Text Analysis is growing in popularity just about everywhere - it’s an abundant source of data that until recently most analysts have ignored because of it’s unwieldy structure. Recent developments and packages in the R programming language have made it easy to derive significant meaning from a text corpus.
Practical Applications of Text Analysis  Analysing email data (e.g. from a customer service inbox to analyze queries and feedback), Web scraping (e.g. Cambridge Analytica, though always consider the ethical implications of such methods even if the data is publicly available or rightfully yours to share), Literary analysis (e.g. poems (Shelley vs Byron) and speeches (Obama vs Trump))  In this post we’ll look at the Australian Election Speeches 1901 - 2016 dataset provided by https://electionspeeches.moadoph.gov.au/speeches - we’ll connect R to their API to read the data, then we’ll clean, analyze and visualize the data with some helpful R packages.
library(jsonlite) library(readr) library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union  Connecting to the API Running the line below will pull the speeches data directly from the API of the Museum of Australian Democracy</description>
    </item>
    
    <item>
      <title>Maps with R and Leaflet</title>
      <link>/blog/2018-07/maps-with-r-and-leaflet/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/maps-with-r-and-leaflet/</guid>
      <description>Interactive Maps in R with leaflet 2018-07-24
Plotting interactive maps is easy with the leaflet package in R. In this short guide we’ll use the Launceston Public Seating dataset to plot markers, add pop-ups and freeze the map on an area of interest. Let’s get started.
Load the leaflet and dplyr libraries:
library(leaflet) library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union Now lets look at our dataset.
Launceston Seats On the 20th of July, 2018, the City of Launceston, Tasmania released the ‘Public Seating’ dataset containing 1011 public seats complete with geo-coordinates, materials (aluminium, timber, etc.) and type (seat with back, seat without back, bench, etc.)
The dataset can be found here https://data.gov.au/dataset/public-seating-1c0ab
Import the data into your R environment:
seats_data &amp;lt;- read.csv(url(&amp;quot;http://data-launceston.opendata.arcgis.com/datasets/e4c2e98c2aed4be688ccf9e901deb84a_0.csv&amp;quot;)) Call str() on the dataset and take a look and what we’ve got (i’m ommitting the 13th variable in the dataset because it’s an obscurely layered list of metadata - it won’t be useful for our purposes):
str(seats_data[,1:12]) ## &amp;#39;data.frame&amp;#39;: 990 obs. of 12 variables: ## $ X : num 147 147 147 147 147 .</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/blog/2015-07/hello-r-markdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/blog/2015-07/hello-r-markdown/</guid>
      <description>R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.
You can embed an R code chunk like this:
 </description>
    </item>
    
    <item>
      <title></title>
      <link>/blog/1-01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/1-01/</guid>
      <description>Text Analysis is growing in popularity just about everywhere - it’s an abundant source of data that until recently most analysts have ignored because of it’s unwieldy structure. Recent developments and packages in the R programming language have made it easy to derive significant meaning from a text corpus.
Practical Applications of Text Analysis  Analysing email data (e.g. from a customer service inbox to analyze queries and feedback), Web scraping (e.g. Cambridge Analytica, though always consider the ethical implications of such methods even if the data is publicly available or rightfully yours to share), Literary analysis (e.g. poems (Shelley vs Byron) and speeches (Obama vs Trump))  In this post we’ll look at the Australian Election Speeches 1901 - 2016 dataset provided by https://electionspeeches.moadoph.gov.au/speeches - we’ll connect R to their API to read the data, then we’ll clean, analyze and visualize the data with some helpful R packages.
library(jsonlite) library(readr) library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union  Connecting to the API Running the line below will pull the speeches data directly from the API of the Museum of Australian Democracy</description>
    </item>
    
  </channel>
</rss>