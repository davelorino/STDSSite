<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Davide</title>
    <link>/</link>
    <description>Recent content on Davide</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-Hans</language>
    <lastBuildDate>Sat, 06 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>STDS Reflection - Scraping the Australian Energy Market Operator &#39;Energy Demand&#39; Data</title>
      <link>/blog/2018-10/scraping-the-australian-energy-market-operator-energy-demand-data/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10/scraping-the-australian-energy-market-operator-energy-demand-data/</guid>
      <description>In this post we will examine how to download all of the data energy demand data available from the Australian Energy Market Operator by using their aemo package in R. This data is displayed via a graphic on their page, however the graphic only allows for a specific timeframe of 5 days to be viewed at a time, and only one state at a time too. Thankfully the aemo package will let us download all of it! Let’s load the necessary libraries.
library(parsedate) library(aemo) library(tidyverse) library(gridExtra) library(plotly) library(caret) library(htmldeps) The most important function we will use from the aemo package is the collate_aemo_data() function. This will start the process of downloading the data to a directory of your choice.
collate_aemo_data(path = &amp;quot;~/Downloads/New Folder With Items&amp;quot;, remove_files = FALSE) Unfortunately for us, the function does this in the form of different csv files for every single day within the dataset (there are close to 500 csv files that are downloaded). There is also a remove files argument in the collate_aemo_data call, and this is because there used to be a function in the aemo package that would bind the csv files together into one csv file and then delete all of the smaller ones with remove files = TRUE.</description>
    </item>
    
    <item>
      <title>Automate the Browser with Selenium and Python</title>
      <link>/blog/2018-08/automate-the-browser-with-selenium-and-python/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-08/automate-the-browser-with-selenium-and-python/</guid>
      <description>img1_path &amp;lt;- &amp;quot;~/Downloads/den/public/images/Scrape.png&amp;quot; library(png) img1 &amp;lt;- readPNG(img1_path, native = TRUE, info = TRUE) Scrape
 Q: How do you scrape data off a website that’s rendered in Javascript?  A: The Selenium module in Python. In this post I will go over he can use Selenium in Python to interact with website objects that are rendered in javascript so that we can do things like:
 Hover-over dropdown menus Select sub-menus Fill in text entry fields such as username and password to perform login operations Basically use the .click() and .find_element_by_xpath() functions to do everything  Building a webscraper/crawler to programmatically pull data from the internet is a really commonly sought after skill. Most businesspeople put together reports that are usually some sort of aggregation on multiple csv files that all need to get pooled together and joined in a summarized form.
Because of the iterative nature of these reports (daily, weekly, monthly, quarterly) the process of requesting and pooling the data together can be extremely tedious, especially if you have lots of data in different places.
For this, we need the Selenium module in Python. We can pre-generate a script that will fetch our data each and every time we want to run a report - the only thing that we will change in our script between iterations will be the date variables so that we arent just pulling data for the exact same date or range every time (we will want different data otherwise theres no point doing more than one of the same report).</description>
    </item>
    
    <item>
      <title>Maps with R and Leaflet</title>
      <link>/blog/2018-07/maps-with-r-and-leaflet/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07/maps-with-r-and-leaflet/</guid>
      <description>Interactive Maps in R with leaflet 2018-07-24
Plotting interactive maps is easy with the leaflet package in R. In this short guide we’ll use the Launceston Public Seating dataset to plot markers, add pop-ups and freeze the map on an area of interest. Let’s get started.
Load the leaflet and dplyr libraries:
library(leaflet) library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union Now lets look at our dataset.
Launceston Seats On the 20th of July, 2018, the City of Launceston, Tasmania released the ‘Public Seating’ dataset containing 1011 public seats complete with geo-coordinates, materials (aluminium, timber, etc.) and type (seat with back, seat without back, bench, etc.)
The dataset can be found here https://data.gov.au/dataset/public-seating-1c0ab
Import the data into your R environment:
seats_data &amp;lt;- read.csv(url(&amp;quot;http://data-launceston.opendata.arcgis.com/datasets/e4c2e98c2aed4be688ccf9e901deb84a_0.csv&amp;quot;)) Call str() on the dataset and take a look and what we’ve got (i’m ommitting the 13th variable in the dataset because it’s an obscurely layered list of metadata - it won’t be useful for our purposes):
str(seats_data[,1:12]) ## &amp;#39;data.frame&amp;#39;: 990 obs. of 12 variables: ## $ X : num 147 147 147 147 147 .</description>
    </item>
    
    <item>
      <title>Moment Test</title>
      <link>/moment/test/</link>
      <pubDate>Wed, 19 Aug 2015 20:29:37 -0700</pubDate>
      
      <guid>/moment/test/</guid>
      <description>The Hitchhiker&amp;rsquo;s Guide to the Galaxy is a comedy science fiction series created by Douglas Adams. Originally a radio comedy broadcast on BBC Radio 4 in 1978, it was later adapted to other formats, and over several years it gradually became an international multi-media phenomenon. Adaptations have included stage shows, a &amp;ldquo;trilogy&amp;rdquo; of five books published between 1979 and 1992, a sixth novel penned by Eoin Colfer in 2009, a 1981 TV series, a 1984 computer game, and three series of three-part comic book adaptations of the first three novels published by DC Comics between 1993 and 1996. There were also two series of towels, produced by Beer-Davies, that are considered by some fans to be an &amp;ldquo;official version&amp;rdquo; of The Hitchhiker&amp;rsquo;s Guide to the Galaxy, as they include text from the first novel.
Movie A Hollywood-funded film version, produced and filmed in the UK, was released in April 2005, and radio adaptations of the third, fourth, and fifth novels were broadcast from 2004 to 2005. Adams did many of these adaptations, including the novels, the TV series, the computer game, and the earliest drafts of the Hollywood film’s screenplay, and some of the stage shows introduced new material written by Adams.</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/blog/2015-07/hello-r-markdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/blog/2015-07/hello-r-markdown/</guid>
      <description>R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.
You can embed an R code chunk like this:
 </description>
    </item>
    
  </channel>
</rss>